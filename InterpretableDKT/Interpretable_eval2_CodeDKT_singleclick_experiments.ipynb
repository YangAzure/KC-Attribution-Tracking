{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1802ffa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yang/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:531: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629416375/work/torch/csrc/utils/python_arg_parser.cpp:766.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run done: 0\n",
      "Run done: 1\n",
      "Run done: 2\n",
      "Run done: 3\n",
      "Run done: 4\n",
      "avg auc: 0.6956763698630137 std auc: 0.01598334109515766 stderr auc: 0.00799167054757883\n",
      "avg acc: 0.6705596107055961 std acc: 0.0203334135433345 stderr acc: 0.01016670677166725\n",
      "avg f1: 0.6751671427667183 std f1: 0.02855690256192617 stderr f1: 0.014278451280963084\n",
      "avg precision: 0.7098040256939567 std precision: 0.01025428706095233 stderr precision: 0.005127143530476165\n",
      "avg recall: 0.6447488584474885 std recall: 0.04430871039528112 stderr recall: 0.02215435519764056\n"
     ]
    }
   ],
   "source": [
    "# Model without synthetic data\n",
    "# Specify arguments\n",
    "\n",
    "# 439\n",
    "PROB_DICT = {1:1,\n",
    "            3:2,\n",
    "            5:3,\n",
    "            12:4,\n",
    "            13:5,\n",
    "            232:6,\n",
    "            233:7,\n",
    "            234:8,\n",
    "            235:9,\n",
    "            236:10\n",
    "           }\n",
    "\n",
    "# # 487\n",
    "# PROB_DICT = {17:1,\n",
    "#             20:2,\n",
    "#             21:3,\n",
    "#             22:4,\n",
    "#             24:5,\n",
    "#             25:6,\n",
    "#             28:7,\n",
    "#             100:8,\n",
    "#             101:9,\n",
    "#             102:10\n",
    "#            }\n",
    "\n",
    "# # 492\n",
    "# PROB_DICT = {31:1,\n",
    "#             32:2,\n",
    "#             33:3,\n",
    "#             34:4,\n",
    "#             36:5,\n",
    "#             37:6,\n",
    "#             38:7,\n",
    "#             39:8,\n",
    "#             40:9,\n",
    "#             128:10\n",
    "#            }\n",
    "\n",
    "# # 494\n",
    "# PROB_DICT = {41:1,\n",
    "#             43:2,\n",
    "#             44:3,\n",
    "#             46:4,\n",
    "#             49:5,\n",
    "#             67:6,\n",
    "#             104:7,\n",
    "#             106:8,\n",
    "#             107:9,\n",
    "#             108:10\n",
    "#            }\n",
    "\n",
    "# # 502\n",
    "# PROB_DICT = {45:1,\n",
    "#             48:2,\n",
    "#             51:3,\n",
    "#             56:4,\n",
    "#             57:5,\n",
    "#             64:6,\n",
    "#             70:7,\n",
    "#             71:8,\n",
    "#             112:9,\n",
    "#             118:10\n",
    "#            }\n",
    "\n",
    "asmt = 439\n",
    "\n",
    "# lrs = [0.0003, 0.0004]\n",
    "# max_epochs_list = [40, 60]\n",
    "# sampling_times = 2\n",
    "# folds = 3\n",
    "\n",
    "lr, max_epochs = 0.01, 120\n",
    "sampling_times = 5\n",
    "folds = 4\n",
    "\n",
    "track_lrs = []\n",
    "track_epochs = []\n",
    "track_aucs = []\n",
    "        \n",
    "\n",
    "batch_size = 64 \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "kc_df = pd.read_csv(\"../data/CWOSyntheticNewProbSolProblem.csv\")\n",
    "\n",
    "tracking_df = kc_df\n",
    "tracking_df = tracking_df.fillna(0).set_index('ProblemID')\n",
    "tracking_df = tracking_df.drop(['ProblemDec', 'Hussle in ChatGPT'], axis=1)\n",
    "dropping = []\n",
    "for c in tracking_df.columns:\n",
    "    column = tracking_df[c]\n",
    "    if sum(column)/len(column)<0.25 or sum(column)/len(column)>0.75:\n",
    "        dropping.append(c)\n",
    "\n",
    "\n",
    "tracking_df = tracking_df.drop(dropping, axis=1)\n",
    "\n",
    "rand_seed = 0\n",
    "code_df = pd.read_csv(\"../data/paths_with_functions.tsv\", sep='\\t')\n",
    "\n",
    "code_df = code_df[code_df['AssignmentID'] == asmt] \n",
    "\n",
    "correct_df = code_df[code_df['Score'] == 1]\n",
    "\n",
    "problems = pd.unique(code_df['ProblemID'])\n",
    "# synthetic_problems = [p for p in problems if p > 1000]\n",
    "# original_problems = [p for p in problems if p < 1000]\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import random\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.stats import sem\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def setup_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "setup_seed(rand_seed)\n",
    "\n",
    "def create_word_index_table(vocab):\n",
    "    \"\"\"\n",
    "    Creating word to index table\n",
    "    Input:\n",
    "    vocab: list. The list of the node vocabulary\n",
    "\n",
    "    \"\"\"\n",
    "    ixtoword = {}\n",
    "    # period at the end of the sentence. make first dimension be end token\n",
    "    ixtoword[0] = 'END'\n",
    "    ixtoword[1] = 'UNK'\n",
    "    wordtoix = {}\n",
    "    wordtoix['END'] = 0\n",
    "    wordtoix['UNK'] = 1\n",
    "    ix = 2\n",
    "    for w in vocab:\n",
    "        wordtoix[w] = ix\n",
    "        ixtoword[ix] = w\n",
    "        ix += 1\n",
    "    return wordtoix, ixtoword\n",
    "\n",
    "def convert_to_idx(sample, node_word_index, path_word_index):\n",
    "    \"\"\"\n",
    "    Converting to the index \n",
    "    Input:\n",
    "    sample: list. One single training sample, which is a code, represented as a list of neighborhoods.\n",
    "    node_word_index: dict. The node to word index dictionary.\n",
    "    path_word_index: dict. The path to word index dictionary.\n",
    "\n",
    "    \"\"\"\n",
    "    sample_index = []\n",
    "    for line in sample:\n",
    "        components = line.split(\",\")\n",
    "        if components[0] in node_word_index:\n",
    "            starting_node = node_word_index[components[0]]\n",
    "        else:\n",
    "            starting_node = node_word_index['UNK']\n",
    "        if components[1] in path_word_index:\n",
    "            path = path_word_index[components[1]]\n",
    "        else:\n",
    "            path = path_word_index['UNK']\n",
    "        if components[2] in node_word_index:\n",
    "            ending_node = node_word_index[components[2]]\n",
    "        else:\n",
    "            ending_node = node_word_index['UNK']\n",
    "\n",
    "        sample_index.append([starting_node,path,ending_node])\n",
    "    return sample_index\n",
    "\n",
    "\n",
    "# Pre-processing of the sequential data\n",
    "# Cheat removal\n",
    "all_students = pd.unique(code_df['SubjectID'])\n",
    "code_df['ServerTimestamp'] = pd.to_datetime(code_df['ServerTimestamp'], format=\"%Y-%m-%dT%H:%M:%S\")\n",
    "def sec_diff(late, early):\n",
    "    return (late - early)/ pd.Timedelta(seconds=1)\n",
    "\n",
    "\n",
    "processed_df_list = []\n",
    "for s in all_students:\n",
    "    if pd.isna(s):\n",
    "        continue\n",
    "    student_df = code_df[code_df['SubjectID'] == s].reset_index()\n",
    "    past_row = student_df.iloc[0]\n",
    "    start_row = student_df.iloc[0]\n",
    "    dropping_index = []\n",
    "    cheat_flag = 0\n",
    "    problem_count = 1\n",
    "    current_problem_attempts = 1\n",
    "    attempt_average = 0\n",
    "    for i, row in student_df.iterrows():\n",
    "        # Init removal time\n",
    "        remove_flag = 0\n",
    "\n",
    "        # Calculate different in submission time\n",
    "        second_diff = sec_diff(row['ServerTimestamp'], past_row['ServerTimestamp'])\n",
    "\n",
    "        # Cheat handling\n",
    "        # New problem\n",
    "        if row['ProblemID'] != past_row['ProblemID'] and i > 0:\n",
    "            # Cheat checker (simple version)\n",
    "\n",
    "            if (current_problem_attempts < 3 and # Not struggled\n",
    "                attempt_average - current_problem_attempts > 8 and # Historical struggle\n",
    "                problem_count > 3):\n",
    "\n",
    "                time_used = sec_diff(past_row['ServerTimestamp'], start_row['ServerTimestamp'])\n",
    "                if time_used < 90: # Short and fast submission\n",
    "                    cheat_flag = 1\n",
    "                    break\n",
    "\n",
    "            # Update to new problem\n",
    "            attempt_average = (attempt_average*problem_count + current_problem_attempts)/(problem_count + 1)\n",
    "            problem_count += 1\n",
    "            current_problem_attempts = 1\n",
    "            start_row = row\n",
    "        # Old problem\n",
    "        else:\n",
    "            current_problem_attempts += 1\n",
    "\n",
    "\n",
    "#         # Confusion handling\n",
    "#         if i > 0:\n",
    "#             if row['Code'] == past_row['Code']:\n",
    "#                 remove_flag = 1\n",
    "#             if second_diff < 15 and row['Score'] != 1:\n",
    "#                 remove_flag = 1\n",
    "\n",
    "        # Partial handling (really simple)\n",
    "        if len(row['Code'].split(\"\\n\")) <= 5:\n",
    "            remove_flag = 1\n",
    "\n",
    "        # Update row\n",
    "        past_row = row\n",
    "\n",
    "        if remove_flag == 1:\n",
    "            dropping_index.append(i)\n",
    "    student_df = student_df.drop(dropping_index)\n",
    "    if not cheat_flag:\n",
    "        processed_df_list.append(student_df)\n",
    "#     processed_df_list.append(student_df)\n",
    "processed_df = pd.concat(processed_df_list)\n",
    "\n",
    "import javalang\n",
    "def program_parser(func):\n",
    "    tokens = javalang.tokenizer.tokenize(func)\n",
    "    parser = javalang.parser.Parser(tokens)\n",
    "    tree = parser.parse_member_declaration()\n",
    "    return tree\n",
    "\n",
    "# Keep first\n",
    "first_df_list = []\n",
    "students = pd.unique(processed_df[\"SubjectID\"])\n",
    "for s in students:\n",
    "    student_df = processed_df[processed_df['SubjectID'] == s].reset_index()\n",
    "    past_ps = []\n",
    "    dropping_index = []\n",
    "    for i, row in student_df.iterrows():\n",
    "        remove_flag = 0\n",
    "\n",
    "        # ONLY KEEP FIRST ATTEMPT!\n",
    "        if row['ProblemID'] not in past_ps:\n",
    "            try:\n",
    "                parsed = program_parser(row['Code'])\n",
    "                past_ps.append(row['ProblemID'])\n",
    "            except:\n",
    "                parsed = \"Uncompilable\"\n",
    "                remove_flag = 1\n",
    "        else:\n",
    "            remove_flag = 1\n",
    "\n",
    "\n",
    "        # Update row\n",
    "        past_row = row\n",
    "\n",
    "        if remove_flag == 1:\n",
    "            dropping_index.append(i)\n",
    "    student_df = student_df.drop(dropping_index)\n",
    "    first_df_list.append(student_df)\n",
    "first_df = pd.concat(first_df_list)\n",
    "\n",
    "# processed_df['First'] = processed_df['CodeStateID'].isin(first_df['CodeStateID'])\n",
    "\n",
    "processed_df = first_df # Only keep the first attempts\n",
    "\n",
    "\n",
    "def read_seq_df(df, subjects, PROB_DICT):\n",
    "    \"\"\"\n",
    "    Reading extracted path data frame and return the paths and the label.\n",
    "\n",
    "    \"\"\"\n",
    "#     separated_paths_label = []\n",
    "    separated_paths_data = []\n",
    "    separated_paths_problem = []\n",
    "    separated_paths_score = []\n",
    "    for s in subjects:\n",
    "        student_df = df[df['SubjectID'] == s]\n",
    "#         s_separated_paths_label = []\n",
    "        s_separated_paths_data = []\n",
    "        s_separated_paths_problem = []\n",
    "        s_separated_paths_score = []\n",
    "        for index, row in student_df.iterrows():\n",
    "            if type(row['RawASTPath']) == str:\n",
    "#                 s_separated_paths_label.append(row[tracking_df.columns[KC_index]])\n",
    "                s_separated_paths_data.append(row['RawASTPath'].split(\"\\U0001f972\"))\n",
    "                index_prob_id = PROB_DICT[row['ProblemID']]\n",
    "                s_separated_paths_problem.append(index_prob_id)\n",
    "                s_separated_paths_score.append(row['Score'])\n",
    "\n",
    "#         separated_paths_label.append(s_separated_paths_label)\n",
    "        separated_paths_data.append(s_separated_paths_data)\n",
    "        separated_paths_problem.append(s_separated_paths_problem)\n",
    "        separated_paths_score.append(s_separated_paths_score)\n",
    "\n",
    "\n",
    "    return separated_paths_data, separated_paths_problem, separated_paths_score\n",
    "\n",
    "\n",
    "def read_seq_df_test(df, subjects, PROB_DICT):\n",
    "    \"\"\"\n",
    "    Reading extracted path data frame and return the paths and the label.\n",
    "\n",
    "    \"\"\"\n",
    "    separated_paths_label = []\n",
    "    separated_paths_data = []\n",
    "    separated_paths_problem = []\n",
    "    separated_paths_score = []\n",
    "    separated_paths_subjects = []\n",
    "    for s in subjects:\n",
    "        student_df = df[df['SubjectID'] == s]\n",
    "        s_separated_paths_label = []\n",
    "        s_separated_paths_data = []\n",
    "        s_separated_paths_problem = []\n",
    "        s_separated_paths_score = []\n",
    "        for index, row in student_df.iterrows():\n",
    "            if type(row['RawASTPath']) == str:\n",
    "                s_separated_paths_label.append(row[tracking_df.columns[:3]])\n",
    "                s_separated_paths_data.append(row['RawASTPath'].split(\"\\U0001f972\"))\n",
    "                index_prob_id = PROB_DICT[row['ProblemID']]\n",
    "                s_separated_paths_problem.append(index_prob_id)\n",
    "                s_separated_paths_score.append(row['Score'])\n",
    "\n",
    "#         \n",
    "        if len(s_separated_paths_data) > 0:\n",
    "            separated_paths_label.append(s_separated_paths_label)\n",
    "            separated_paths_data.append(s_separated_paths_data)\n",
    "            separated_paths_problem.append(s_separated_paths_problem)\n",
    "            separated_paths_score.append(s_separated_paths_score)\n",
    "            separated_paths_subjects.append(s)\n",
    "\n",
    "\n",
    "    return separated_paths_data, separated_paths_problem, separated_paths_score, separated_paths_subjects, separated_paths_label\n",
    "\n",
    "\n",
    "class CodeDKT(nn.Module):\n",
    "    \"\"\"\n",
    "    Defining the network. \n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, node_count, path_count, P_number, seq_dim, tracking_df, device):\n",
    "        super(CodeDKT, self).__init__()\n",
    "        KC_number = len(tracking_df.columns)\n",
    "\n",
    "        self.KC_filter = torch.FloatTensor(tracking_df[tracking_df.index<1000].to_numpy()).to(device)\n",
    "        self.seq_dim = seq_dim\n",
    "        \n",
    "        self.embed_nodes = nn.Embedding(node_count+2, 100) # adding unk and end\n",
    "        self.embed_paths = nn.Embedding(path_count+2, 100) # adding unk and end\n",
    "        self.embed_dropout = nn.Dropout(0.2)\n",
    "        self.path_transformation_layer = nn.Linear(seq_dim+300,seq_dim+300)\n",
    "        self.attention_layer = nn.Linear(seq_dim+300,1)\n",
    "\n",
    "        self.attention_softmax = nn.Softmax(dim=2)\n",
    "\n",
    "#         self.rnn = nn.LSTM(2*seq_dim+300, 100, batch_first=True) \n",
    "        self.rnn1 = nn.LSTM(seq_dim, 100, batch_first=True) # Without code\n",
    "        self.rnn2 = nn.LSTM(seq_dim+300, 100, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(200, KC_number) # With code\n",
    "#         self.fc = nn.Linear(100, KC_number) # Without code\n",
    "        self.KC_pred = nn.Linear(KC_number, 10)\n",
    "        self.leakyReLU = nn.LeakyReLU()\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.device = device\n",
    "    def forward(self, starting_node_index, ending_node_index, path_index, correctness_sequence, evaluating=False):\n",
    "        '''\n",
    "        Parameters:\n",
    "        batch: b\n",
    "        codes: c (non-unified)\n",
    "        node_embeddings: ne\n",
    "        path_embeddings: pe\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        rnn_first_part = correctness_sequence # (b,l,2q)\n",
    "\n",
    "        rnn_attention_part = torch.stack([rnn_first_part.float()]*200,dim=-2) # (b,l,c,2q)\n",
    "        starting_node_embed = self.embed_nodes(starting_node_index) # (b,l,c,1) -> (b,l,c,ne)\n",
    "        ending_node_embed = self.embed_nodes(ending_node_index) # (b,l,c,1) -> (b,l,c,ne)\n",
    "        path_embed = self.embed_paths(path_index) # (b,l,c,1) -> (b,l,c,pe)\n",
    "        full_embed = torch.cat((starting_node_embed, ending_node_embed, path_embed, rnn_attention_part), dim=3) # (b,l,c,2ne+pe+q)\n",
    "        if not evaluating:\n",
    "            full_embed = self.embed_dropout(full_embed) # (b,l,c,2ne+pe+2q)\n",
    "        full_embed_transformed = torch.tanh(self.path_transformation_layer(full_embed)) # (b,l,c,2ne+pe+2q)\n",
    "        context_weights = self.attention_layer(full_embed_transformed) # (b,l,c,1)\n",
    "        attention_weights = self.attention_softmax(context_weights) # (b,l,c,1)\n",
    "        code_vectors = torch.sum(torch.mul(full_embed,attention_weights),dim=2) # (b,l,2ne+pe+2q)\n",
    "        \n",
    "        rnn_input = torch.cat((rnn_first_part.float(),code_vectors), dim=2)\n",
    "        \n",
    "#         out, hn = self.rnn(rnn_input) \n",
    "        out1, hn1 = self.rnn1(rnn_first_part.float())\n",
    "        out2, hn2 = self.rnn2(code_vectors)\n",
    "    \n",
    "        KC_res = self.sig(self.fc(torch.cat((out1,out2), dim=2))) # With code\n",
    "#         KC_res = self.sig(self.fc(out1)) # Without code\n",
    "        \n",
    "        # Keeping only positive weights for KCs (now is a special softmax)\n",
    "        self.KC_pred.weight = nn.Parameter(self.sig(self.KC_pred.weight.data))\n",
    "        self.KC_pred.weight = nn.Parameter(torch.mul(self.KC_pred.weight, self.KC_filter))\n",
    "        self.KC_pred.weight = nn.Parameter(nn.functional.normalize(self.KC_pred.weight.data, dim=0))\n",
    "        \n",
    "        res = self.sig(self.KC_pred(KC_res)) \n",
    "        \n",
    "        return res, KC_res\n",
    "\n",
    "    def get_minibatches_idx(self, n, minibatch_size, shuffle=False, droplast=False):\n",
    "        \"\"\"\n",
    "        Getting the minibatches given the training set\n",
    "\n",
    "        \"\"\"\n",
    "        idx_list = np.arange(len(n))\n",
    "\n",
    "        if shuffle:\n",
    "            np.random.shuffle(idx_list)\n",
    "\n",
    "        minibatches = []\n",
    "        minibatch_start = 0\n",
    "        if droplast:\n",
    "            for i in range(math.floor(len(n) // minibatch_size)):\n",
    "                minibatches.append(idx_list[minibatch_start : min(minibatch_start + minibatch_size,len(n))])\n",
    "                minibatch_start += minibatch_size\n",
    "        else:\n",
    "            for i in range(math.ceil(len(n) / minibatch_size)):\n",
    "                minibatches.append(idx_list[minibatch_start : min(minibatch_start + minibatch_size,len(n))])\n",
    "                minibatch_start += minibatch_size\n",
    "        return zip(range(len(minibatches)), minibatches)\n",
    "\n",
    "#     def padding(self, index_list, max_length):\n",
    "#         \"\"\"\n",
    "#         Padding the paths and the code.\n",
    "\n",
    "#         \"\"\"\n",
    "#         padder_index = 0\n",
    "\n",
    "#         padded_list = []\n",
    "#         for sample in index_list:\n",
    "\n",
    "#             if max_length > len(sample):\n",
    "#                 # padding the paths, or just get the first \"max_gram_length\" of paths\n",
    "#                 padding_length = max_length - len(sample)\n",
    "#                 padded_list.append(sample + [padder_index]*padding_length)\n",
    "#             else:\n",
    "#                 padded_list.append(sample[:max_length])\n",
    "\n",
    "#         return padded_list\n",
    "\n",
    "#     def padding_sequence(self, student_list, num_questions):\n",
    "#         max_length = 100\n",
    "#         padder_index = [0]*max_length\n",
    "#         padded_list = []\n",
    "#         for index_list in student_list:\n",
    "\n",
    "#             if num_questions > len(index_list):\n",
    "#                 # padding the paths, or just get the first \"max_gram_length\" of paths\n",
    "#                 padding_length = num_questions - len(index_list)\n",
    "#                 padded_list.append(self.padding(index_list, max_length) + [padder_index]*padding_length)\n",
    "#             else:\n",
    "#                 padded_list.append(self.padding(index_list[:num_questions], max_length))\n",
    "\n",
    "#         return padded_list\n",
    "\n",
    "    def save_model(self, save_path):\n",
    "        torch.save(self.state_dict(), save_path)\n",
    "\n",
    "    def load_model(self, load_path):\n",
    "        self.load_state_dict(torch.load(load_path))\n",
    "\n",
    "def zeros2end(x):\n",
    "    zeros_mask = x == 0\n",
    "    out = torch.stack(\n",
    "        [torch.cat([x[_, :][~zeros_mask[_, :]], x[_, :][zeros_mask[_, :]]], dim=0) for _ in range(x.size()[0])])\n",
    "    return out\n",
    "\n",
    "class lossFunc(nn.Module):\n",
    "    def __init__(self, num_of_questions, bs, device):\n",
    "        super(lossFunc, self).__init__()\n",
    "        self.crossEntropy = nn.BCELoss()\n",
    "        self.MSE = nn.MSELoss()\n",
    "        self.num_of_questions = num_of_questions\n",
    "        self.batch_size = bs\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, pred, correctness, problems, device=\"cuda:0\"):\n",
    "\n",
    "        binary_preds = torch.tensor([])\n",
    "        ground_truths = torch.tensor([])\n",
    "\n",
    "        list_preds = []\n",
    "        for s_ind in range(len(pred)):\n",
    "            padded_len = len(torch.nonzero(problems[s_ind]==0))\n",
    "            extra = padded_len\n",
    "\n",
    "#             print(s_ind, padded_len)\n",
    "            for j in range(49-extra): # We cannot count the final submission in.\n",
    "                current_pred = pred[s_ind,j].to('cpu')\n",
    "\n",
    "                next_correctness = (((correctness[s_ind,j+1,:10] - correctness[s_ind,j+1,10:])+1)//2).to('cpu')\n",
    "                problem_attempted_index = problems[s_ind,j+1]-1\n",
    "\n",
    "#                 print(\"-----\")\n",
    "#                 print(\"next correct vec:\", next_correctness)\n",
    "#                 print(\"prob index:\", problem_attempted_index)\n",
    "#                 print(\"next correct:\", next_correctness[problem_attempted_index])\n",
    "#                 print(\"pred:\", current_pred[problem_attempted_index])\n",
    "\n",
    "#                 print(\"-----\")\n",
    "                binary_preds = torch.cat([binary_preds, torch.unsqueeze(current_pred[problem_attempted_index], 0)]) \n",
    "                ground_truths = torch.cat([ground_truths, torch.unsqueeze(next_correctness[problem_attempted_index],0)])\n",
    "        loss = self.crossEntropy(binary_preds.double(), ground_truths.double())\n",
    "        return loss, binary_preds, ground_truths\n",
    "\n",
    "#     def forward_ori(self, pred, correctness, problems, device=\"cuda:0\"):\n",
    "\n",
    "\n",
    "#         binary_preds = torch.tensor([]).to(device)\n",
    "#         ground_truths = torch.tensor([]).to(device)\n",
    "\n",
    "#         for s_ind in range(len(pred)):\n",
    "\n",
    "#             padded_len = len(torch.nonzero(problems[s_ind]==0))\n",
    "#             extra = padded_len\n",
    "\n",
    "#             delta = correctness[s_ind][:, 0:10] + correctness[s_ind][:, 10:20]  # shape: [length, questions]\n",
    "#             temp = pred[s_ind][:49].mm(delta[1:].t().float())\n",
    "#             index = torch.tensor([[i for i in range(49)]],dtype=torch.long).to(device)\n",
    "\n",
    "#             p = temp.gather(0, index)[0]\n",
    "#             a = (((correctness[s_ind][:, 0:10] - correctness[s_ind][:, 10:20]).sum(1) + 1) // 2)[1:]\n",
    "\n",
    "#             p = p[:49-extra]\n",
    "#             a = a[:49-extra]\n",
    "\n",
    "#             binary_preds = torch.cat([binary_preds, p])\n",
    "#             ground_truths = torch.cat([ground_truths, a])\n",
    "\n",
    "#         loss = self.crossEntropy(binary_preds, ground_truths)\n",
    "#         return loss, binary_preds, ground_truths\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "label1_df = pd.read_csv(\"../data/439_yang_labeling_data_1.csv\")\n",
    "label2_df = pd.read_csv(\"../data/439_yang_labeling_data_2.csv\")\n",
    "\n",
    "label_df = pd.concat([label1_df, label2_df])\n",
    "\n",
    "problems = pd.unique(processed_df['ProblemID'])\n",
    "synthetic_problems = [p for p in problems if p > 1000]\n",
    "original_problems = [p for p in problems if p < 1000]\n",
    "\n",
    "subjects = pd.unique(processed_df['SubjectID'])\n",
    "\n",
    "\n",
    "test_subjects = pd.unique(label_df['SubjectID'])\n",
    "train_val_subjects = pd.unique(processed_df[~processed_df['SubjectID'].isin(test_subjects)]['SubjectID'])\n",
    "\n",
    "original_processed_df = processed_df[processed_df['ProblemID'].isin(original_problems)]\n",
    "original_processed_df = original_processed_df.sort_values('ServerTimestamp')\n",
    "test_df = original_processed_df[original_processed_df['SubjectID'].isin(test_subjects)]\n",
    "test_df = test_df[test_df['CodeStateID'].isin(label_df['CodeStateID'])]\n",
    "\n",
    "for c in tracking_df.columns:\n",
    "    test_df[c] = [1-label_df[label_df['CodeStateID'] == x[1]['CodeStateID']][c].iloc[0] for x in test_df.iterrows()]\n",
    "    \n",
    "    \n",
    "# synth_df = pd.DataFrame(columns=test_df.columns)\n",
    "    \n",
    "# for s in test_subjects:\n",
    "#     subject_df = test_df[test_df['SubjectID']==s]\n",
    "#     non_subject_df = test_df[test_df['SubjectID']!=s]\n",
    "#     for i in range(len(subject_df)):\n",
    "#         row = subject_df.iloc[i]\n",
    "#         if row['Score'] == 1:\n",
    "#             to_replace_p = row['ProblemID']\n",
    "\n",
    "#             KCs_related = tracking_df.loc[to_replace_p].tolist()\n",
    "#             KCs_index = [n for n, e in enumerate(KCs_related) if e == 1]\n",
    "#             for k in KCs_index:\n",
    "#                 target_df = non_subject_df[non_subject_df['ProblemID'] == to_replace_p]\n",
    "#                 target_df = target_df[target_df[tracking_df.columns.to_list()[k]] == 0]\n",
    "#                 for k_other in KCs_index:\n",
    "#                     if k_other != k:\n",
    "#                         target_df = target_df[target_df[tracking_df.columns.to_list()[k_other]] == 1]\n",
    "#                 if len(target_df) == 0:\n",
    "#                     continue\n",
    "#                 replaced_row = target_df.sample().copy()\n",
    "\n",
    "                \n",
    "#                 for j, row_j in subject_df.iloc[:i].iterrows():\n",
    "#                     new_row_j = row_j.copy()\n",
    "#                     new_row_j['SubjectID'] = s + \"_seq\" + str(i) + \"_kc\" + str(k)\n",
    "#                     synth_df = synth_df.append(new_row_j)\n",
    "                \n",
    "#                 replaced_row['ServerTimestamp'] = \"2024-01-01 20:00:00\" # Assign a super late time to ensure last one\n",
    "#                 replaced_row['SubjectID'] = s + \"_seq\" + str(i) + \"_kc\" + str(k)\n",
    "#                 synth_df = synth_df.append(replaced_row)\n",
    "\n",
    "# synth_subjects = pd.unique(synth_df['SubjectID'])\n",
    "\n",
    "# # Keeping only 75 pairs per kc.\n",
    "# kc0_subjects = [e for e  in synth_subjects if e[-1] == '0'][:75]\n",
    "# kc1_subjects = [e for e in synth_subjects if e[-1] == '1'][:75]\n",
    "# kc2_subjects = [e for e in synth_subjects if e[-1] == '2'][:75]\n",
    "\n",
    "# synth_df = synth_df[synth_df['SubjectID'].isin(kc0_subjects+kc1_subjects+kc2_subjects)]\n",
    "\n",
    "\n",
    "test_f1 = []\n",
    "test_auc = []\n",
    "test_acc = []\n",
    "test_precision = []\n",
    "test_recall = []\n",
    "# synth_interp_values = []\n",
    "# test_interp_values = []\n",
    "# synth_interp_values_by_kc = []\n",
    "# test_interp_values_by_kc = []\n",
    "# interp_acc = []\n",
    "test_KC_vectors = []\n",
    "test_gt_vectors = []\n",
    "# synth_KC_vectors = []\n",
    "# KC_evaled = []\n",
    "\n",
    "\n",
    "practiced_problems = list(PROB_DICT.keys())\n",
    "\n",
    "\n",
    "\n",
    "for i in range(sampling_times):\n",
    "    \n",
    "    \n",
    "#     original_processed_df = processed_df[processed_df['ProblemID'].isin(original_problems)]\n",
    "    processed_df = processed_df.sort_values('ServerTimestamp')\n",
    "    \n",
    "    train_df = processed_df[processed_df['SubjectID'].isin(train_val_subjects)]\n",
    "    \n",
    "#     for c in tracking_df.columns:\n",
    "#         train_df[c] = [tracking_df.loc[x[1]['ProblemID'], c] for x in train_df.iterrows()]\n",
    "#         val_df[c] = [tracking_df.loc[x[1]['ProblemID'], c] for x in val_df.iterrows()] # Readin labels\n",
    "\n",
    "    train_data, train_problem, train_score = read_seq_df(train_df, train_val_subjects, PROB_DICT)\n",
    "    test_data, test_problem, test_score, test_subjects_new, test_kc_label = read_seq_df_test(test_df, test_subjects, PROB_DICT)\n",
    "#     synth_data, synth_problem, synth_score, synth_subjects_new = read_seq_df_test(synth_df, synth_subjects, PROB_DICT)\n",
    "\n",
    "\n",
    "    node_hist = {}\n",
    "    path_hist = {}\n",
    "    for student_paths in train_data:\n",
    "        for paths in student_paths:\n",
    "            starting_nodes = [p.split(\",\")[0] for p in paths]\n",
    "            path = [p.split(\",\")[1] for p in paths]\n",
    "            ending_nodes = [p.split(\",\")[2] for p in paths]\n",
    "            nodes = starting_nodes + ending_nodes\n",
    "            for n in nodes:\n",
    "                if not n in node_hist:\n",
    "                    node_hist[n] = 1\n",
    "                else:\n",
    "                    node_hist[n] += 1\n",
    "            for p in path:\n",
    "                if not p in path_hist:\n",
    "                    path_hist[p] = 1\n",
    "                else:\n",
    "                    path_hist[p] += 1\n",
    "\n",
    "    node_count = len(node_hist)\n",
    "    path_count = len(path_hist)\n",
    "\n",
    "    # small frequency then abandon, for node and path\n",
    "    valid_node = [node for node, count in node_hist.items()]\n",
    "    valid_path = [path for path, count in path_hist.items()]\n",
    "\n",
    "    # create ixtoword and wordtoix lists\n",
    "    node_word_index, node_index_word = create_word_index_table(valid_node)\n",
    "    path_word_index, path_index_word = create_word_index_table(valid_path)\n",
    "    \n",
    "    train_converted_data = [[convert_to_idx(sample, node_word_index, path_word_index) for sample in student_sample] for student_sample in train_data]\n",
    "    test_converted_data = [[convert_to_idx(sample, node_word_index, path_word_index) for sample in student_sample] for student_sample  in test_data]\n",
    "#     synth_converted_data = [[convert_to_idx(sample, node_word_index, path_word_index) for sample in student_sample] for student_sample  in synth_data]\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_full_info = np.zeros(shape=[len(train_val_subjects), 50, 620]) # \n",
    "    test_full_info = np.zeros(shape=[len(test_subjects_new), 50, 620])\n",
    "#     synth_full_info = np.zeros(shape=[len(synth_subjects_new), 50, 620])\n",
    "    \n",
    "    for s_ind in range(len(train_val_subjects)):\n",
    "        current_s_attempts = len(train_score[s_ind])\n",
    "        \n",
    "        if current_s_attempts > 50:\n",
    "            train_score[s_ind] = train_score[s_ind][:50]\n",
    "            train_problem[s_ind] = train_problem[s_ind][:50]\n",
    "            train_converted_data[s_ind] = train_converted_data[s_ind][:50]\n",
    "\n",
    "#         extra = 50 - min(current_s_attempts, 50)\n",
    "        for j in range(min(current_s_attempts, 50)):\n",
    "            \n",
    "            p_count = len(train_converted_data[s_ind][j])\n",
    "\n",
    "            if p_count > 200:\n",
    "                train_full_info[s_ind, j, :200] = [p[0] for p in train_converted_data[s_ind][j][:200]] # Starting\n",
    "                train_full_info[s_ind, j, 200:400] = [p[2] for p in train_converted_data[s_ind][j][:200]] # Ending\n",
    "                train_full_info[s_ind, j, 400:600] = [p[1] for p in train_converted_data[s_ind][j][:200]] # Path\n",
    "            else:\n",
    "                train_full_info[s_ind, j, :p_count] = [p[0] for p in train_converted_data[s_ind][j]] # Starting\n",
    "                train_full_info[s_ind, j, 200:200+p_count] = [p[2] for p in train_converted_data[s_ind][j]] # Ending\n",
    "                train_full_info[s_ind, j, 400:400+p_count] = [p[1] for p in train_converted_data[s_ind][j]] # Path\n",
    "                \n",
    "            if train_score[s_ind][j] == 1:\n",
    "                # 0 index should be for the first problem, however used for padding in model.\n",
    "                train_full_info[s_ind, j, 600+train_problem[s_ind][j]-1] = 1 \n",
    "            else:\n",
    "                train_full_info[s_ind, j, 600+train_problem[s_ind][j]+9] = 1\n",
    "    \n",
    "    for s_ind in range(len(test_subjects_new)):\n",
    "        current_s_attempts = len(test_score[s_ind])\n",
    "        \n",
    "        if current_s_attempts > 50:\n",
    "            test_score[s_ind] = test_score[s_ind][:50]\n",
    "            test_problem[s_ind] = test_problem[s_ind][:50]\n",
    "            test_converted_data[s_ind] = test_converted_data[s_ind][:50]\n",
    "        \n",
    "#         extra = 50 - min(current_s_attempts, 50)\n",
    "        for j in range(min(current_s_attempts, 50)):\n",
    "        \n",
    "            p_count = len(test_converted_data[s_ind][j])\n",
    "        \n",
    "            if p_count > 200:\n",
    "                test_full_info[s_ind, j, :200] = [p[0] for p in test_converted_data[s_ind][j][:200]] # Starting\n",
    "                test_full_info[s_ind, j, 200:400] = [p[2] for p in test_converted_data[s_ind][j][:200]] # Ending\n",
    "                test_full_info[s_ind, j, 400:600] = [p[1] for p in test_converted_data[s_ind][j][:200]] # Path\n",
    "            else:\n",
    "                test_full_info[s_ind, j, :p_count] = [p[0] for p in test_converted_data[s_ind][j]] # Starting\n",
    "                test_full_info[s_ind, j, 200:200+p_count] = [p[2] for p in test_converted_data[s_ind][j]] # Ending\n",
    "                test_full_info[s_ind, j, 400:400+p_count] = [p[1] for p in test_converted_data[s_ind][j]] # Path\n",
    "            \n",
    "            if test_score[s_ind][j] == 1:\n",
    "                test_full_info[s_ind, j, 600+test_problem[s_ind][j]-1] = 1\n",
    "            else:\n",
    "                test_full_info[s_ind, j, 600+test_problem[s_ind][j]+9] = 1\n",
    "                \n",
    "                \n",
    "#     for s_ind in range(len(synth_subjects_new)):\n",
    "#         current_s_attempts = len(synth_score[s_ind])\n",
    "        \n",
    "#         if current_s_attempts > 50:\n",
    "#             synth_score[s_ind] = synth_score[s_ind][:50]\n",
    "#             synth_problem[s_ind] = synth_problem[s_ind][:50]\n",
    "#             synth_converted_data[s_ind] = synth_converted_data[s_ind][:50]\n",
    "        \n",
    "# #         extra = 50 - min(current_s_attempts, 50)\n",
    "#         for j in range(min(current_s_attempts, 50)):\n",
    "        \n",
    "#             p_count = len(synth_converted_data[s_ind][j])\n",
    "        \n",
    "#             if p_count > 200:\n",
    "#                 synth_full_info[s_ind, j, :200] = [p[0] for p in synth_converted_data[s_ind][j][:200]] # Starting\n",
    "#                 synth_full_info[s_ind, j, 200:400] = [p[2] for p in synth_converted_data[s_ind][j][:200]] # Ending\n",
    "#                 synth_full_info[s_ind, j, 400:600] = [p[1] for p in synth_converted_data[s_ind][j][:200]] # Path\n",
    "#             else:\n",
    "#                 synth_full_info[s_ind, j, :p_count] = [p[0] for p in synth_converted_data[s_ind][j]] # Starting\n",
    "#                 synth_full_info[s_ind, j, 200:200+p_count] = [p[2] for p in synth_converted_data[s_ind][j]] # Ending\n",
    "#                 synth_full_info[s_ind, j, 400:400+p_count] = [p[1] for p in synth_converted_data[s_ind][j]] # Path\n",
    "            \n",
    "#             if synth_score[s_ind][j] == 1:\n",
    "#                 synth_full_info[s_ind, j, 600+synth_problem[s_ind][j]-1] = 1\n",
    "#             else:\n",
    "#                 synth_full_info[s_ind, j, 600+synth_problem[s_ind][j]+9] = 1\n",
    "    \n",
    "#     val_converted_data += train_converted_data\n",
    "    \n",
    "    P_number = len(problems)\n",
    "    \n",
    "    model = CodeDKT(node_count, path_count, P_number, 20, tracking_df, device).to(device)\n",
    "    \n",
    "    loss_func = lossFunc(10, batch_size, device).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        mini_batches = model.get_minibatches_idx(train_converted_data, batch_size, shuffle=True, droplast=False)\n",
    "        epoch_loss = 0\n",
    "        for _, batch_index in mini_batches:\n",
    "            \n",
    "#             batch_samples = [train_converted_data[t] for t in batch_index]\n",
    "#             raw_starting_node_index = [[[p[0] for p in single_sample] for single_sample in student_samples] for student_samples in batch_samples]\n",
    "#             raw_ending_node_index = [[[p[2] for p in single_sample] for single_sample in student_samples] for student_samples in batch_samples]\n",
    "#             raw_path_index = [[[p[1] for p in single_sample] for single_sample in student_samples] for student_samples in batch_samples]\n",
    "#             batch_max_len = 50\n",
    "#             # Padding to max paths in a file\n",
    "#             starting_node_index = torch.LongTensor(model.padding_sequence(raw_starting_node_index, num_questions=batch_max_len)).to(device)\n",
    "#             ending_node_index = torch.LongTensor(model.padding_sequence(raw_ending_node_index, num_questions=batch_max_len)).to(device)\n",
    "#             path_index = torch.LongTensor(model.padding_sequence(raw_path_index, num_questions=batch_max_len)).to(device)\n",
    "            \n",
    "            batch_scores = [train_score[t] for t in batch_index]\n",
    "        \n",
    "            batch_problem = np.zeros(shape=[len(batch_index), 50])\n",
    "            for ind, t in enumerate(batch_index):\n",
    "                if len(train_problem[t]) > 50:\n",
    "                    batch_problem[ind,:] = train_problem[t][:50]\n",
    "                else:\n",
    "                    batch_problem[ind,:len(train_problem[t])] = train_problem[t]\n",
    "                    \n",
    "            batch_problem = torch.LongTensor(batch_problem)\n",
    "            \n",
    "            batch_correctness = torch.nn.utils.rnn.pad_sequence([torch.tensor(train_full_info[t,:,600:]) for t in batch_index], batch_first=True, padding_value=0).to(device)\n",
    "\n",
    "            starting_node_index = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(train_full_info[t,:,:200]) for t in batch_index], batch_first=True, padding_value=0).to(device)\n",
    "            ending_node_index = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(train_full_info[t,:,200:400]) for t in batch_index], batch_first=True, padding_value=0).to(device)\n",
    "            path_index = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(train_full_info[t,:,400:600]) for t in batch_index], batch_first=True, padding_value=0).to(device)\n",
    "            \n",
    "            predictions, code_vectors = model(starting_node_index, ending_node_index, path_index, batch_correctness)\n",
    "\n",
    "            loss, binary_preds, ground_truths = loss_func(predictions, batch_correctness, batch_problem)\n",
    "            \n",
    "            epoch_loss += loss.item()*len(batch_index)\n",
    "            # Model optimization\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_loss = epoch_loss / len(train_converted_data)\n",
    "        \n",
    "    #test\n",
    "    model.eval()\n",
    "    test_mini_batches = model.get_minibatches_idx(test_converted_data, min(batch_size,len(test_converted_data)), shuffle=False)\n",
    "    test_epoch_loss = 0\n",
    "    test_epoch_lc_loss = 0\n",
    "    \n",
    "    test_binary_preds = torch.tensor([])\n",
    "    test_ground_truths = torch.tensor([])\n",
    "    test_code_vectors = []\n",
    "    test_raw_preds = []\n",
    "    for _, test_batch_index in test_mini_batches:\n",
    "\n",
    "        batch_scores = [test_score[t] for t in test_batch_index]\n",
    "\n",
    "        batch_problem = np.zeros(shape=[len(test_batch_index), 50])\n",
    "        for ind, t in enumerate(test_batch_index):\n",
    "            if len(test_problem[t]) > 50:\n",
    "                batch_problem[ind,:] = test_problem[t][:50]\n",
    "            else:\n",
    "                batch_problem[ind,:len(test_problem[t])] = test_problem[t]\n",
    "\n",
    "        batch_problem = torch.LongTensor(batch_problem)\n",
    "\n",
    "        batch_correctness = torch.nn.utils.rnn.pad_sequence([torch.tensor(test_full_info[t,:,600:]) for t in test_batch_index], batch_first=True, padding_value=0).to(device)\n",
    "\n",
    "        starting_node_index = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(test_full_info[t,:,:200]) for t in test_batch_index], batch_first=True, padding_value=0).to(device)\n",
    "        ending_node_index = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(test_full_info[t,:,200:400]) for t in test_batch_index], batch_first=True, padding_value=0).to(device)\n",
    "        path_index = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(test_full_info[t,:,400:600]) for t in test_batch_index], batch_first=True, padding_value=0).to(device)\n",
    "        with torch.no_grad():\n",
    "            predictions, code_vectors = model(starting_node_index, ending_node_index, path_index, batch_correctness, evaluating=True)\n",
    "\n",
    "            loss, binary_preds, ground_truths = loss_func(predictions, batch_correctness, batch_problem)\n",
    "\n",
    "#             print(binary_preds, ground_truths)\n",
    "        test_binary_preds = torch.cat([test_binary_preds, binary_preds]) \n",
    "        test_ground_truths = torch.cat([test_ground_truths, ground_truths])\n",
    "        test_epoch_loss += loss.item()*len(test_batch_index)\n",
    "        test_code_vectors.extend(code_vectors)\n",
    "        test_raw_preds.extend(predictions)\n",
    "\n",
    "    # Calculating the prediction scores and the classification results.\n",
    "    test_loss = test_epoch_loss / len(test_converted_data)\n",
    "\n",
    "    test_auc.append(metrics.roc_auc_score(test_ground_truths.cpu().detach().numpy(), (test_binary_preds).cpu().detach().numpy()))\n",
    "    test_acc.append(metrics.accuracy_score(test_ground_truths.cpu().detach().numpy(), (test_binary_preds).cpu().detach().numpy().round())) \n",
    "    test_f1.append(metrics.f1_score(test_ground_truths.cpu().detach().numpy(), (test_binary_preds).cpu().detach().numpy().round()))\n",
    "    test_precision.append(metrics.precision_score(test_ground_truths.cpu().detach().numpy(), (test_binary_preds).cpu().detach().numpy().round()))\n",
    "    test_recall.append(metrics.recall_score(test_ground_truths.cpu().detach().numpy(), (test_binary_preds).cpu().detach().numpy().round()))\n",
    "    \n",
    "#     #synth\n",
    "#     synth_mini_batches = model.get_minibatches_idx(synth_converted_data, min(batch_size,len(synth_converted_data)), shuffle=False)\n",
    "#     synth_epoch_loss = 0\n",
    "#     synth_epoch_lc_loss = 0\n",
    "    \n",
    "#     synth_binary_preds = torch.tensor([])\n",
    "#     synth_ground_truths = torch.tensor([])\n",
    "#     synth_code_vectors = []\n",
    "#     synth_raw_preds = []\n",
    "#     for _, synth_batch_index in synth_mini_batches:\n",
    "\n",
    "#         batch_scores = [synth_score[t] for t in synth_batch_index]\n",
    "\n",
    "#         batch_problem = np.zeros(shape=[len(synth_batch_index), 50])\n",
    "#         for ind, t in enumerate(synth_batch_index):\n",
    "#             if len(synth_problem[t]) > 50:\n",
    "#                 batch_problem[ind,:] = synth_problem[t][:50]\n",
    "#             else:\n",
    "#                 batch_problem[ind,:len(synth_problem[t])] = synth_problem[t]\n",
    "\n",
    "#         batch_problem = torch.LongTensor(batch_problem)\n",
    "\n",
    "#         batch_correctness = torch.nn.utils.rnn.pad_sequence([torch.tensor(synth_full_info[t,:,600:]) for t in synth_batch_index], batch_first=True, padding_value=0).to(device)\n",
    "\n",
    "#         starting_node_index = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(synth_full_info[t,:,:200]) for t in synth_batch_index], batch_first=True, padding_value=0).to(device)\n",
    "#         ending_node_index = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(synth_full_info[t,:,200:400]) for t in synth_batch_index], batch_first=True, padding_value=0).to(device)\n",
    "#         path_index = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(synth_full_info[t,:,400:600]) for t in synth_batch_index], batch_first=True, padding_value=0).to(device)\n",
    "#         with torch.no_grad():\n",
    "#             predictions, code_vectors = model(starting_node_index, ending_node_index, path_index, batch_correctness, evaluating=True)\n",
    "\n",
    "#             loss, binary_preds, ground_truths = loss_func(predictions, batch_correctness, batch_problem)\n",
    "\n",
    "# #             print(binary_preds, ground_truths)\n",
    "#         synth_binary_preds = torch.cat([synth_binary_preds, binary_preds]) \n",
    "#         synth_ground_truths = torch.cat([synth_ground_truths, ground_truths])\n",
    "# #         synth_epoch_loss += loss.item()*len(synth_batch_index)\n",
    "#         synth_code_vectors.extend(code_vectors)\n",
    "#         synth_raw_preds.extend(predictions)\n",
    "    # Check: Synth traces (subject ID) should be matching with number of preds and truths\n",
    "#     test_KC_vals = []\n",
    "#     synth_KC_vals = []\n",
    "    test_KC_vec = []\n",
    "    test_gt_vec = []\n",
    "#     target_KCs = []\n",
    "#     synth_interp_values_by_kc_dict = {0:[],1:[],2:[]}\n",
    "#     test_interp_values_by_kc_dict = {0:[],1:[],2:[]}\n",
    "#     diff_to_probs = []\n",
    "    \n",
    "    for s_ind in range(len(test_subjects_new)):\n",
    "#         split_subject = synth_subjects_new[s_ind].split(\"_\")\n",
    "#         trace_subject = split_subject[0]\n",
    "#         seq_num = int(split_subject[1][-1])\n",
    "#         kc_ind = int(split_subject[2][-1])\n",
    "#         test_ind = list(test_subjects_new).index(trace_subject)\n",
    "#         print(trace_subject, seq_num, kc_ind, len(synth_code_vectors))\n",
    "        \n",
    "        attempt_N = len(list(test_df[test_df['SubjectID'] == test_subjects_new[s_ind]]['ProblemID']))\n",
    "\n",
    "        test_KC_vec.append([test_code_vectors[s_ind].tolist()[att] for att in range(attempt_N)])\n",
    "        test_gt_vec.append([test_kc_label[s_ind][att] for att in range(attempt_N)])\n",
    "        \n",
    "    test_KC_vectors.append(test_KC_vec)\n",
    "    test_gt_vectors.append(test_gt_vec)\n",
    "#         print(test_code_vectors[test_ind])\n",
    "#         print(synth_code_vectors[s_ind])\n",
    "        \n",
    "#         test_KC_vals.append(test_KC_val)\n",
    "#         synth_KC_vals.append(synth_KC_val)\n",
    "#         test_interp_values_by_kc_dict[kc_ind].append(test_KC_val)\n",
    "#         synth_interp_values_by_kc_dict[kc_ind].append(synth_KC_val)\n",
    "#         target_KCs.append(kc_ind)\n",
    "        \n",
    "#         test_KC_vec.append(test_code_vectors[test_ind][seq_num].tolist())\n",
    "#         synth_KC_vec.append(synth_code_vectors[s_ind][seq_num].tolist())\n",
    "        \n",
    "#     test_KC_vectors.append(test_KC_vec)\n",
    "#     test_gt_vectors.append(test_gt_vec)\n",
    "# #     synth_interp_values.append(synth_KC_vals)\n",
    "# #     test_interp_values.append(test_KC_vals)\n",
    "    \n",
    "#     test_interp_values_by_kc.append(test_interp_values_by_kc_dict)\n",
    "#     synth_interp_values_by_kc.append(synth_interp_values_by_kc_dict)\n",
    "#     KC_evaled.append(target_KCs)\n",
    "    \n",
    "    \n",
    "#     for v_ind in range(len(test_KC_vals)):\n",
    "#         diff_to_probs.append((test_KC_vals[v_ind] - synth_KC_vals[v_ind])>0)\n",
    "        \n",
    "#     interp_acc.append(sum(diff_to_probs)/len(diff_to_probs)) \n",
    "#     print(interp_acc)\n",
    "    print(\"Run done:\", i)\n",
    "\n",
    "# print(\"avg interp acc:\", np.mean(interp_acc), \"std interp acc:\", np.std(interp_acc), \"stderr interp acc:\", sem(interp_acc))\n",
    "\n",
    "print(\"avg auc:\", np.mean(test_auc), \"std auc:\", np.std(test_auc), \"stderr auc:\", sem(test_auc))\n",
    "print(\"avg acc:\", np.mean(test_acc), \"std acc:\", np.std(test_acc), \"stderr acc:\", sem(test_acc))\n",
    "print(\"avg f1:\", np.mean(test_f1), \"std f1:\", np.std(test_f1), \"stderr f1:\", sem(test_f1))\n",
    "print(\"avg precision:\", np.mean(test_precision), \"std precision:\", np.std(test_precision), \"stderr precision:\", sem(test_precision))\n",
    "print(\"avg recall:\", np.mean(test_recall), \"std recall:\", np.std(test_recall), \"stderr recall:\", sem(test_recall))            \n",
    "\n",
    "\n",
    "\n",
    "# kc_val_diffs_evaled = []\n",
    "# kc_val_diffs_unevaled = []\n",
    "\n",
    "# kc_val_pos_portion_evaled = []\n",
    "# kc_val_pos_portion_unevaled = []\n",
    "\n",
    "# for f in range(sampling_times):\n",
    "#     test_KC_vecs = test_KC_vectors[f]\n",
    "#     synth_KC_vecs = synth_KC_vectors[f]\n",
    "#     target_KCs = KC_evaled[f]\n",
    "#     diff_evaled = []\n",
    "#     diff_unevaled = []\n",
    "#     pos_evaled = []\n",
    "#     pos_unevaled = []\n",
    "#     for v_ind in range(len(target_KCs)):\n",
    "#         for k in range(3):\n",
    "#             if k == target_KCs[v_ind]:\n",
    "#                 diff_evaled.append(test_KC_vecs[v_ind][k] - synth_KC_vecs[v_ind][k])\n",
    "#                 pos_evaled.append((test_KC_vecs[v_ind][k] - synth_KC_vecs[v_ind][k])>0)\n",
    "#             else:\n",
    "#                 diff_unevaled.append(test_KC_vecs[v_ind][k] - synth_KC_vecs[v_ind][k])\n",
    "#                 pos_unevaled.append((test_KC_vecs[v_ind][k] - synth_KC_vecs[v_ind][k])>0)\n",
    "#     kc_val_diffs_evaled.append(np.mean(diff_evaled))\n",
    "#     kc_val_diffs_unevaled.append(np.mean(diff_unevaled))\n",
    "#     kc_val_pos_portion_evaled.append(sum(pos_evaled)/len(pos_evaled))\n",
    "#     kc_val_pos_portion_unevaled.append(sum(pos_unevaled)/len(pos_unevaled))\n",
    "    \n",
    "# print(\"np.mean(kc_val_diffs_unevaled):\",np.mean(kc_val_diffs_unevaled))\n",
    "# print(\"np.mean(kc_val_pos_portion_unevaled):\",np.mean(kc_val_pos_portion_unevaled))\n",
    "\n",
    "# problems_attempted = []\n",
    "# for s_ind in range(len(synth_subjects_new)):\n",
    "#     split_subject = synth_subjects_new[s_ind].split(\"_\")\n",
    "#     problems_attempted.append(list(synth_df[synth_df['SubjectID'] == synth_subjects_new[s_ind]]['ProblemID'])[-1])\n",
    "    \n",
    "# kc_val_diffs_evaled = []\n",
    "# kc_val_diffs_unevaled = []\n",
    "# kc_val_diffs_unrelated = []\n",
    "\n",
    "# kc_val_pos_portion_evaled = []\n",
    "# kc_val_pos_portion_unevaled = []\n",
    "# kc_val_pos_portion_unrelated = []\n",
    "\n",
    "# kc_val_diffs_evaled_split = {0:[], 1:[], 2:[]}\n",
    "# kc_val_diffs_unevaled_split = {0:[], 1:[], 2:[]}\n",
    "# kc_val_diffs_unrelated_split = {0:[], 1:[], 2:[]}\n",
    "\n",
    "# kc_val_pos_portion_evaled_split = {0:[], 1:[], 2:[]}\n",
    "# kc_val_pos_portion_unevaled_split = {0:[], 1:[], 2:[]}\n",
    "# kc_val_pos_portion_unrelated_split = {0:[], 1:[], 2:[]}\n",
    "\n",
    "# for f in range(sampling_times):\n",
    "#     test_KC_vecs = test_KC_vectors[f]\n",
    "#     synth_KC_vecs = synth_KC_vectors[f]\n",
    "#     target_KCs = KC_evaled[f]\n",
    "    \n",
    "#     problem_changed = problems_attempted\n",
    "    \n",
    "#     diff_evaled = []\n",
    "#     diff_unevaled = []\n",
    "#     diff_unrelated = []\n",
    "#     pos_evaled = []\n",
    "#     pos_unevaled = []\n",
    "#     pos_unrelated = []\n",
    "    \n",
    "#     diff_evaled_kc = {0:[], 1:[], 2:[]}\n",
    "#     diff_unevaled_kc = {0:[], 1:[], 2:[]}\n",
    "#     diff_unrelated_kc = {0:[], 1:[], 2:[]}\n",
    "#     pos_evaled_kc = {0:[], 1:[], 2:[]}\n",
    "#     pos_unevaled_kc = {0:[], 1:[], 2:[]}\n",
    "#     pos_unrelated_kc = {0:[], 1:[], 2:[]}\n",
    "    \n",
    "#     for v_ind in range(len(target_KCs)):\n",
    "#         for k in range(3):\n",
    "#             if k == target_KCs[v_ind]:\n",
    "#                 diff_evaled.append(test_KC_vecs[v_ind][k] - synth_KC_vecs[v_ind][k])\n",
    "#                 pos_evaled.append((test_KC_vecs[v_ind][k] - synth_KC_vecs[v_ind][k])>0)\n",
    "#                 diff_evaled_kc[k].append(test_KC_vecs[v_ind][k] - synth_KC_vecs[v_ind][k])\n",
    "#                 pos_evaled_kc[k].append((test_KC_vecs[v_ind][k] - synth_KC_vecs[v_ind][k])>0)\n",
    "#             elif k in [i for i, e in enumerate(tracking_df.loc[problems_attempted[v_ind]].tolist()) if e == 1]:\n",
    "#                 diff_unevaled.append(test_KC_vecs[v_ind][k] - synth_KC_vecs[v_ind][k])\n",
    "#                 pos_unevaled.append((test_KC_vecs[v_ind][k] - synth_KC_vecs[v_ind][k])>0)\n",
    "#                 diff_unevaled_kc[k].append(test_KC_vecs[v_ind][k] - synth_KC_vecs[v_ind][k])\n",
    "#                 pos_unevaled_kc[k].append((test_KC_vecs[v_ind][k] - synth_KC_vecs[v_ind][k])>0)\n",
    "#             else:\n",
    "#                 diff_unrelated.append(test_KC_vecs[v_ind][k] - synth_KC_vecs[v_ind][k])\n",
    "#                 pos_unrelated.append((test_KC_vecs[v_ind][k] - synth_KC_vecs[v_ind][k])>0)\n",
    "#                 diff_unrelated_kc[k].append(test_KC_vecs[v_ind][k] - synth_KC_vecs[v_ind][k])\n",
    "#                 pos_unrelated_kc[k].append((test_KC_vecs[v_ind][k] - synth_KC_vecs[v_ind][k])>0)\n",
    "#     kc_val_diffs_evaled.append(np.mean(diff_evaled))\n",
    "#     kc_val_diffs_unevaled.append(np.mean(diff_unevaled))\n",
    "#     kc_val_diffs_unrelated.append(np.mean(diff_unrelated))\n",
    "#     kc_val_pos_portion_evaled.append(sum(pos_evaled)/len(pos_evaled))\n",
    "#     kc_val_pos_portion_unevaled.append(sum(pos_unevaled)/len(pos_unevaled))\n",
    "#     kc_val_pos_portion_unrelated.append(sum(pos_unrelated)/len(pos_unrelated))\n",
    "    \n",
    "#     for k in range(3):\n",
    "#         kc_val_diffs_evaled_split[k].append(np.mean(diff_evaled_kc[k]))\n",
    "#         kc_val_diffs_unevaled_split[k].append(np.mean(diff_unevaled_kc[k]))\n",
    "#         kc_val_diffs_unrelated_split[k].append(np.mean(diff_unrelated_kc[k]))\n",
    "#         kc_val_pos_portion_evaled_split[k].append(sum(pos_evaled_kc[k])/len(pos_evaled_kc[k]))\n",
    "#         kc_val_pos_portion_unevaled_split[k].append(sum(pos_unevaled_kc[k])/len(pos_unevaled_kc[k]))\n",
    "#         kc_val_pos_portion_unrelated_split[k].append(sum(pos_unrelated_kc[k])/len(pos_unrelated_kc[k]))\n",
    "    \n",
    "    \n",
    "# print(\"np.mean(kc_val_diffs_evaled):\",np.mean(kc_val_diffs_evaled))   \n",
    "# print(\"np.mean(kc_val_diffs_unevaled):\",np.mean(kc_val_diffs_unevaled))\n",
    "# print(\"np.mean(kc_val_diffs_unrelated):\",np.mean(kc_val_diffs_unrelated))\n",
    "# print(\"np.mean(kc_val_pos_portion_evaled):\",np.mean(kc_val_pos_portion_evaled))   \n",
    "# print(\"np.mean(kc_val_pos_portion_unevaled):\",np.mean(kc_val_pos_portion_unevaled))\n",
    "# print(\"np.mean(kc_val_pos_portion_unrelated):\",np.mean(kc_val_pos_portion_unrelated))\n",
    "\n",
    "# for k in range(3):\n",
    "#     print(\"KC:\", k)\n",
    "#     print(\"np.mean(kc_val_diffs_evaled):\",np.mean(kc_val_diffs_evaled_split[k]))   \n",
    "#     print(\"np.mean(kc_val_diffs_unevaled):\",np.mean(kc_val_diffs_unevaled_split[k]))\n",
    "#     print(\"np.mean(kc_val_diffs_unrelated):\",np.mean(kc_val_diffs_unrelated_split[k]))\n",
    "#     print(\"np.mean(kc_val_pos_portion_evaled):\",np.mean(kc_val_pos_portion_evaled_split[k]))   \n",
    "#     print(\"np.mean(kc_val_pos_portion_unevaled):\",np.mean(kc_val_pos_portion_unevaled_split[k]))\n",
    "#     print(\"np.mean(kc_val_pos_portion_unrelated):\",np.mean(kc_val_pos_portion_unrelated_split[k]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1495bb65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tracking_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-520d115b464e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mkc_gt_related_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mQ_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracking_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtracking_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mproblems_attempted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms_ind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_subjects_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tracking_df' is not defined"
     ]
    }
   ],
   "source": [
    "kc_val_related = []\n",
    "kc_val_unrelated = []\n",
    "kc_gt_related = []\n",
    "kc_gt_unrelated = []\n",
    "\n",
    "kc_val_related_split = {0:[], 1:[], 2:[]}\n",
    "kc_gt_related_split = {0:[], 1:[], 2:[]}\n",
    "\n",
    "Q_matrix = tracking_df[tracking_df.index<1000].to_numpy()\n",
    "problems_attempted = []\n",
    "for s_ind in range(len(test_subjects_new)):\n",
    "    problems_attempted.append(list(test_df[test_df['SubjectID'] == test_subjects_new[s_ind]]['ProblemID']))\n",
    "\n",
    "for f in range(sampling_times):\n",
    "    test_KC_vecs = test_KC_vectors[f]\n",
    "    test_gt_vecs = test_gt_vectors[f]\n",
    "    \n",
    "    problem_changed = problems_attempted\n",
    "    \n",
    "    related = []\n",
    "    unrelated = []\n",
    "    \n",
    "    related_kc = {0:[], 1:[], 2:[]}\n",
    "    unrelated_kc = {0:[], 1:[], 2:[]}\n",
    "    \n",
    "    related_gt = []\n",
    "    unrelated_gt = []\n",
    "    \n",
    "    related_gt_kc = {0:[], 1:[], 2:[]}\n",
    "    unrelated_gt_kc = {0:[], 1:[], 2:[]}\n",
    "    \n",
    "    for s_ind in range(len(test_KC_vecs)):\n",
    "        for v_ind in range(len(test_KC_vecs[s_ind])):\n",
    "            for k in range(3):\n",
    "                if k in [i for i, e in enumerate(tracking_df.loc[problems_attempted[s_ind][v_ind]].tolist()) if e == 1]:\n",
    "                    test_KC_num = test_KC_vecs[s_ind][v_ind][k]\n",
    "                    test_gt_num = test_gt_vecs[s_ind][v_ind][k]\n",
    "                    \n",
    "                    related_gt.append(test_gt_num)\n",
    "                    related.append(test_KC_num)\n",
    "\n",
    "                    related_gt_kc[k].append(test_gt_num)\n",
    "                    related_kc[k].append(test_KC_num)\n",
    "                    \n",
    "                    if len([i for i, e in enumerate(tracking_df.loc[problems_attempted[s_ind][v_ind]].tolist()) if e == 1]) == 1:\n",
    "                        related_gt.append(test_gt_num)\n",
    "                        related.append(test_KC_num)\n",
    "\n",
    "                        related_gt_kc[k].append(test_gt_num)\n",
    "                        related_kc[k].append(test_KC_num)\n",
    "                    \n",
    "                if k in [i for i, e in enumerate(tracking_df.loc[problems_attempted[s_ind][v_ind]].tolist()) if e == 1]:\n",
    "                    if len([i for i, e in enumerate(tracking_df.loc[problems_attempted[s_ind][v_ind]].tolist()) if e == 1]) == 2:\n",
    "                        if test_score[s_ind][v_ind] != 1:\n",
    "                            test_KC_num = test_KC_vecs[s_ind][v_ind][k]\n",
    "                            test_gt_num = test_gt_vecs[s_ind][v_ind][k]\n",
    "\n",
    "                            unrelated_gt.append(test_gt_num)\n",
    "                            unrelated.append(test_KC_num)\n",
    "\n",
    "                            unrelated_gt_kc[k].append(test_gt_num)\n",
    "                            unrelated_kc[k].append(test_KC_num)\n",
    "    kc_val_related.append(related)\n",
    "    kc_val_unrelated.append(unrelated)\n",
    "    kc_gt_related.append(related_gt)\n",
    "    kc_gt_unrelated.append(unrelated_gt)\n",
    "    \n",
    "    \n",
    "    for k in range(3):\n",
    "        kc_val_related_split[k].append(related_kc[k])\n",
    "        kc_gt_related_split[k].append(related_gt_kc[k])\n",
    "\n",
    "auc_related = [metrics.roc_auc_score(kc_gt_related[f], kc_val_related[f]) for f in range(sampling_times)]\n",
    "auc_unrelated = [metrics.roc_auc_score(kc_gt_unrelated[f], kc_val_unrelated[f]) for f in range(sampling_times)]\n",
    "print(np.mean(auc_related), np.std(auc_related), sem(auc_related))\n",
    "print(np.mean(auc_unrelated), np.std(auc_unrelated), sem(auc_unrelated))\n",
    "#     kc_val_diffs_evaled.append(np.mean(diff_evaled))\n",
    "#     kc_val_diffs_unevaled.append(np.mean(diff_unevaled))\n",
    "#     kc_val_diffs_unrelated.append(np.mean(diff_unrelated))\n",
    "#     kc_val_pos_portion_evaled.append(sum(pos_evaled)/len(pos_evaled))\n",
    "#     kc_val_pos_portion_unevaled.append(sum(pos_unevaled)/len(pos_unevaled))\n",
    "#     kc_val_pos_portion_unrelated.append(sum(pos_unrelated)/len(pos_unrelated))\n",
    "    \n",
    "#     for k in range(3):\n",
    "#         kc_val_diffs_evaled_split[k].append(np.mean(diff_evaled_kc[k]))\n",
    "#         kc_val_diffs_unevaled_split[k].append(np.mean(diff_unevaled_kc[k]))\n",
    "#         kc_val_diffs_unrelated_split[k].append(np.mean(diff_unrelated_kc[k]))\n",
    "#         kc_val_pos_portion_evaled_split[k].append(sum(pos_evaled_kc[k])/len(pos_evaled_kc[k]))\n",
    "#         kc_val_pos_portion_unevaled_split[k].append(sum(pos_unevaled_kc[k])/len(pos_unevaled_kc[k]))\n",
    "#         kc_val_pos_portion_unrelated_split[k].append(sum(pos_unrelated_kc[k])/len(pos_unrelated_kc[k]))\n",
    "    \n",
    "for k in range(3):\n",
    "    print(\"KC:\", k)\n",
    "    auc_related = [metrics.roc_auc_score(kc_gt_related_split[k][f], kc_val_related_split[k][f]) for f in range(sampling_times)]\n",
    "    \n",
    "    print(np.mean(auc_related), np.std(auc_related), sem(auc_related))\n",
    "    \n",
    "# print(\"np.mean(kc_val_diffs_evaled):\",np.mean(kc_val_diffs_evaled))   \n",
    "# print(\"np.mean(kc_val_diffs_unevaled):\",np.mean(kc_val_diffs_unevaled))\n",
    "# print(\"np.mean(kc_val_diffs_unrelated):\",np.mean(kc_val_diffs_unrelated))\n",
    "# print(\"np.mean(kc_val_pos_portion_evaled):\",np.mean(kc_val_pos_portion_evaled))   \n",
    "# print(\"np.mean(kc_val_pos_portion_unevaled):\",np.mean(kc_val_pos_portion_unevaled))\n",
    "# print(\"np.mean(kc_val_pos_portion_unrelated):\",np.mean(kc_val_pos_portion_unrelated))\n",
    "\n",
    "# for k in range(3):\n",
    "#     print(\"KC:\", k)\n",
    "#     print(\"np.mean(kc_val_diffs_evaled):\",np.mean(kc_val_diffs_evaled_split[k]))   \n",
    "#     print(\"np.mean(kc_val_diffs_unevaled):\",np.mean(kc_val_diffs_unevaled_split[k]))\n",
    "#     print(\"np.mean(kc_val_diffs_unrelated):\",np.mean(kc_val_diffs_unrelated_split[k]))\n",
    "#     print(\"np.mean(kc_val_pos_portion_evaled):\",np.mean(kc_val_pos_portion_evaled_split[k]))   \n",
    "#     print(\"np.mean(kc_val_pos_portion_unevaled):\",np.mean(kc_val_pos_portion_unevaled_split[k]))\n",
    "#     print(\"np.mean(kc_val_pos_portion_unrelated):\",np.mean(kc_val_pos_portion_unrelated_split[k]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d363a1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yang/anaconda3/envs/py36/lib/python3.6/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": [
    "np.save(\"CodeKCDKT_kcval.npy\",[kc_val_related, kc_val_unrelated, kc_gt_related, kc_gt_unrelated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "735bec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kc_val_diffs_evaled_save = {0:[], 1:[], 2:[]}\n",
    "# kc_val_diffs_unevaled_save = {0:[], 1:[], 2:[]}\n",
    "# kc_val_diffs_unrelated_save = {0:[], 1:[], 2:[]}\n",
    "\n",
    "\n",
    "# for f in range(sampling_times):\n",
    "#     test_KC_vecs = test_KC_vectors[f]\n",
    "#     synth_KC_vecs = synth_KC_vectors[f]\n",
    "#     target_KCs = KC_evaled[f]\n",
    "    \n",
    "#     problem_changed = problems_attempted\n",
    "    \n",
    "#     diff_evaled = []\n",
    "#     diff_unevaled = []\n",
    "#     diff_unrelated = []\n",
    "#     pos_evaled = []\n",
    "#     pos_unevaled = []\n",
    "#     pos_unrelated = []\n",
    "    \n",
    "#     diff_evaled_kc = {0:[], 1:[], 2:[]}\n",
    "#     diff_unevaled_kc = {0:[], 1:[], 2:[]}\n",
    "#     diff_unrelated_kc = {0:[], 1:[], 2:[]}\n",
    "#     pos_evaled_kc = {0:[], 1:[], 2:[]}\n",
    "#     pos_unevaled_kc = {0:[], 1:[], 2:[]}\n",
    "#     pos_unrelated_kc = {0:[], 1:[], 2:[]}\n",
    "    \n",
    "#     for v_ind in range(len(target_KCs)):\n",
    "#         for k in range(3):\n",
    "#             if k == target_KCs[v_ind]:\n",
    "#                 diff_evaled.append(test_KC_vecs[v_ind][k] - synth_KC_vecs[v_ind][k])\n",
    "#                 pos_evaled.append((test_KC_vecs[v_ind][k] - synth_KC_vecs[v_ind][k])>0)\n",
    "#                 diff_evaled_kc[k].append(test_KC_vecs[v_ind][k] - synth_KC_vecs[v_ind][k])\n",
    "#                 pos_evaled_kc[k].append((test_KC_vecs[v_ind][k] - synth_KC_vecs[v_ind][k])>0)\n",
    "#             elif k in [i for i, e in enumerate(tracking_df.loc[problems_attempted[v_ind]].tolist()) if e == 1]:\n",
    "#                 diff_unevaled.append(test_KC_vecs[v_ind][k] - synth_KC_vecs[v_ind][k])\n",
    "#                 pos_unevaled.append((test_KC_vecs[v_ind][k] - synth_KC_vecs[v_ind][k])>0)\n",
    "#                 diff_unevaled_kc[k].append(test_KC_vecs[v_ind][k] - synth_KC_vecs[v_ind][k])\n",
    "#                 pos_unevaled_kc[k].append((test_KC_vecs[v_ind][k] - synth_KC_vecs[v_ind][k])>0)\n",
    "#             else:\n",
    "#                 diff_unrelated.append(test_KC_vecs[v_ind][k] - synth_KC_vecs[v_ind][k])\n",
    "#                 pos_unrelated.append((test_KC_vecs[v_ind][k] - synth_KC_vecs[v_ind][k])>0)\n",
    "#                 diff_unrelated_kc[k].append(test_KC_vecs[v_ind][k] - synth_KC_vecs[v_ind][k])\n",
    "#                 pos_unrelated_kc[k].append((test_KC_vecs[v_ind][k] - synth_KC_vecs[v_ind][k])>0)\n",
    "#     kc_val_diffs_evaled.append(np.mean(diff_evaled))\n",
    "#     kc_val_diffs_unevaled.append(np.mean(diff_unevaled))\n",
    "#     kc_val_diffs_unrelated.append(np.mean(diff_unrelated))\n",
    "#     kc_val_pos_portion_evaled.append(sum(pos_evaled)/len(pos_evaled))\n",
    "#     kc_val_pos_portion_unevaled.append(sum(pos_unevaled)/len(pos_unevaled))\n",
    "#     kc_val_pos_portion_unrelated.append(sum(pos_unrelated)/len(pos_unrelated))\n",
    "    \n",
    "#     for k in range(3):\n",
    "#         kc_val_diffs_evaled_save[k].append(diff_evaled_kc[k])\n",
    "#         kc_val_diffs_unevaled_save[k].append(diff_unevaled_kc[k])\n",
    "#         kc_val_diffs_unrelated_save[k].append(diff_unrelated_kc[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a86fa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"CodeKCDKT_interp.npy\",[kc_val_diffs_evaled_save, kc_val_diffs_unevaled_save, kc_val_diffs_unrelated_save])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9040c19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"KCDKT_usecase.npy\", [test_KC_vec, test_subjects_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84891888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(test_KC_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd8b2777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i, e in enumerate(tracking_df.loc[problems_attempted[s_ind][v_ind]].tolist()) if e == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eec0d3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_KC_vecs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-adc8cd256ccc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcount_2kc_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcount_2kc_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0ms_ind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_KC_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mv_ind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_KC_vecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtracking_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mproblems_attempted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_KC_vecs' is not defined"
     ]
    }
   ],
   "source": [
    "count_1kc = 0\n",
    "count_2kc = 0\n",
    "count_1kc_c = 0\n",
    "count_1kc_i = 0\n",
    "count_2kc_c = 0\n",
    "count_2kc_i = 0\n",
    "for s_ind in range(len(test_KC_vecs)):\n",
    "    for v_ind in range(len(test_KC_vecs[s_ind])):\n",
    "        if len([i for i, e in enumerate(tracking_df.loc[problems_attempted[s_ind][v_ind]].tolist()) if e == 1]) == 2:\n",
    "            count_2kc+=1\n",
    "            if test_score[s_ind][v_ind] != 1:\n",
    "                count_2kc_i+=1\n",
    "            else:\n",
    "                count_2kc_c+=1\n",
    "        else:\n",
    "            count_1kc+=1\n",
    "            if test_score[s_ind][v_ind] != 1:\n",
    "                count_1kc_i+=1\n",
    "            else:\n",
    "                count_1kc_c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8923d8b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
